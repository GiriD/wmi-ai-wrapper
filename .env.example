# WMI Agent Configuration

# Provider Selection
# Options: "ollama" (local) or "azure" (cloud)
AGENT_PROVIDER=ollama

# Ollama Configuration (for local inference)
OLLAMA_MODEL=gpt-oss:120b
OLLAMA_ENDPOINT=http://localhost:11434/v1

# Azure OpenAI Configuration (for cloud inference)
# Uncomment and set these if using Azure provider
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-api-key-here
# AZURE_OPENAI_DEPLOYMENT=gpt-4o
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
